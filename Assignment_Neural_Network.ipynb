{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment Neural Network",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSV1PfDiIM_M"
      },
      "source": [
        "# **Forestfires**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nssQk6ByDu0Y",
        "outputId": "d9ce3cfc-71f3-438c-cc6f-93536d6833c5"
      },
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.5.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.34.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.32.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (57.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (4.6.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xae4rX2YDuyu"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfqhqqUVDuwY"
      },
      "source": [
        "df=pd.read_csv('forestfires.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "d-8omqnWDuvx",
        "outputId": "3586927e-e771-4bce-f311-15a3f40f98df"
      },
      "source": [
        "df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>FFMC</th>\n",
              "      <th>DMC</th>\n",
              "      <th>DC</th>\n",
              "      <th>ISI</th>\n",
              "      <th>temp</th>\n",
              "      <th>RH</th>\n",
              "      <th>wind</th>\n",
              "      <th>rain</th>\n",
              "      <th>area</th>\n",
              "      <th>dayfri</th>\n",
              "      <th>daymon</th>\n",
              "      <th>daysat</th>\n",
              "      <th>daysun</th>\n",
              "      <th>daythu</th>\n",
              "      <th>daytue</th>\n",
              "      <th>daywed</th>\n",
              "      <th>monthapr</th>\n",
              "      <th>monthaug</th>\n",
              "      <th>monthdec</th>\n",
              "      <th>monthfeb</th>\n",
              "      <th>monthjan</th>\n",
              "      <th>monthjul</th>\n",
              "      <th>monthjun</th>\n",
              "      <th>monthmar</th>\n",
              "      <th>monthmay</th>\n",
              "      <th>monthnov</th>\n",
              "      <th>monthoct</th>\n",
              "      <th>monthsep</th>\n",
              "      <th>size_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mar</td>\n",
              "      <td>fri</td>\n",
              "      <td>86.2</td>\n",
              "      <td>26.2</td>\n",
              "      <td>94.3</td>\n",
              "      <td>5.1</td>\n",
              "      <td>8.2</td>\n",
              "      <td>51</td>\n",
              "      <td>6.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>oct</td>\n",
              "      <td>tue</td>\n",
              "      <td>90.6</td>\n",
              "      <td>35.4</td>\n",
              "      <td>669.1</td>\n",
              "      <td>6.7</td>\n",
              "      <td>18.0</td>\n",
              "      <td>33</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>oct</td>\n",
              "      <td>sat</td>\n",
              "      <td>90.6</td>\n",
              "      <td>43.7</td>\n",
              "      <td>686.9</td>\n",
              "      <td>6.7</td>\n",
              "      <td>14.6</td>\n",
              "      <td>33</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mar</td>\n",
              "      <td>fri</td>\n",
              "      <td>91.7</td>\n",
              "      <td>33.3</td>\n",
              "      <td>77.5</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.3</td>\n",
              "      <td>97</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mar</td>\n",
              "      <td>sun</td>\n",
              "      <td>89.3</td>\n",
              "      <td>51.3</td>\n",
              "      <td>102.2</td>\n",
              "      <td>9.6</td>\n",
              "      <td>11.4</td>\n",
              "      <td>99</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>aug</td>\n",
              "      <td>sun</td>\n",
              "      <td>81.6</td>\n",
              "      <td>56.7</td>\n",
              "      <td>665.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>27.8</td>\n",
              "      <td>32</td>\n",
              "      <td>2.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.44</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>aug</td>\n",
              "      <td>sun</td>\n",
              "      <td>81.6</td>\n",
              "      <td>56.7</td>\n",
              "      <td>665.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>21.9</td>\n",
              "      <td>71</td>\n",
              "      <td>5.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>aug</td>\n",
              "      <td>sun</td>\n",
              "      <td>81.6</td>\n",
              "      <td>56.7</td>\n",
              "      <td>665.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>21.2</td>\n",
              "      <td>70</td>\n",
              "      <td>6.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>aug</td>\n",
              "      <td>sat</td>\n",
              "      <td>94.4</td>\n",
              "      <td>146.0</td>\n",
              "      <td>614.7</td>\n",
              "      <td>11.3</td>\n",
              "      <td>25.6</td>\n",
              "      <td>42</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>nov</td>\n",
              "      <td>tue</td>\n",
              "      <td>79.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>106.7</td>\n",
              "      <td>1.1</td>\n",
              "      <td>11.8</td>\n",
              "      <td>31</td>\n",
              "      <td>4.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>517 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    month  day  FFMC    DMC  ...  monthnov  monthoct  monthsep  size_category\n",
              "0     mar  fri  86.2   26.2  ...         0         0         0          small\n",
              "1     oct  tue  90.6   35.4  ...         0         1         0          small\n",
              "2     oct  sat  90.6   43.7  ...         0         1         0          small\n",
              "3     mar  fri  91.7   33.3  ...         0         0         0          small\n",
              "4     mar  sun  89.3   51.3  ...         0         0         0          small\n",
              "..    ...  ...   ...    ...  ...       ...       ...       ...            ...\n",
              "512   aug  sun  81.6   56.7  ...         0         0         0          large\n",
              "513   aug  sun  81.6   56.7  ...         0         0         0          large\n",
              "514   aug  sun  81.6   56.7  ...         0         0         0          large\n",
              "515   aug  sat  94.4  146.0  ...         0         0         0          small\n",
              "516   nov  tue  79.5    3.0  ...         1         0         0          small\n",
              "\n",
              "[517 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B_ARfnCDus-"
      },
      "source": [
        "df1=df.drop(['month','day'],axis=1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qVinUPoDupl"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_EJ0fv_Duoo"
      },
      "source": [
        "lb=LabelEncoder()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aABhtXeEDulw"
      },
      "source": [
        "df1['size_category']=lb.fit_transform(df1['size_category'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwL6lbCyDulB",
        "outputId": "a2ba8719-dd28-4488-ef82-cc8c1087ebcd"
      },
      "source": [
        "df2=df1.values\n",
        "df2.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(517, 29)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cl-Q8qKjDuhX",
        "outputId": "ada1b86e-9a62-4248-9eb3-102dbcdc872a"
      },
      "source": [
        "x=df2[:,0:28]\n",
        "y=df2[:,-1]\n",
        "x.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(517, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeCqL2M6DueY"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBMi9gweDudu"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=28, activation='relu'))\n",
        "model.add(Dense(28, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIGNb4VoDuah"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYRA18P0DuZu",
        "outputId": "2e068202-b947-42f1-f646-fd55e8807282"
      },
      "source": [
        "model.fit(x, y, validation_split=0.33,epochs=100, batch_size=5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "70/70 [==============================] - 15s 9ms/step - loss: 17.2464 - accuracy: 0.3125 - val_loss: 1.0910 - val_accuracy: 0.6842\n",
            "Epoch 2/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6442 - accuracy: 0.7918 - val_loss: 0.7305 - val_accuracy: 0.5673\n",
            "Epoch 3/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7951 - val_loss: 0.6547 - val_accuracy: 0.5906\n",
            "Epoch 4/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4869 - accuracy: 0.7748 - val_loss: 0.4956 - val_accuracy: 0.7602\n",
            "Epoch 5/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.8177 - val_loss: 0.5988 - val_accuracy: 0.7485\n",
            "Epoch 6/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.3585 - accuracy: 0.8508 - val_loss: 0.3938 - val_accuracy: 0.8246\n",
            "Epoch 7/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.2550 - accuracy: 0.8970 - val_loss: 0.3548 - val_accuracy: 0.8538\n",
            "Epoch 8/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.2649 - accuracy: 0.8859 - val_loss: 0.4767 - val_accuracy: 0.8012\n",
            "Epoch 9/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.2152 - accuracy: 0.9131 - val_loss: 0.2928 - val_accuracy: 0.8713\n",
            "Epoch 10/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.2411 - accuracy: 0.8910 - val_loss: 0.3157 - val_accuracy: 0.8480\n",
            "Epoch 11/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1525 - accuracy: 0.9552 - val_loss: 0.3145 - val_accuracy: 0.8538\n",
            "Epoch 12/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1150 - accuracy: 0.9638 - val_loss: 0.2457 - val_accuracy: 0.8889\n",
            "Epoch 13/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1573 - accuracy: 0.9509 - val_loss: 0.2257 - val_accuracy: 0.8889\n",
            "Epoch 14/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1110 - accuracy: 0.9744 - val_loss: 0.2281 - val_accuracy: 0.8830\n",
            "Epoch 15/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0926 - accuracy: 0.9724 - val_loss: 0.2553 - val_accuracy: 0.8713\n",
            "Epoch 16/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0780 - accuracy: 0.9684 - val_loss: 0.1958 - val_accuracy: 0.9123\n",
            "Epoch 17/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0840 - accuracy: 0.9747 - val_loss: 0.2935 - val_accuracy: 0.8772\n",
            "Epoch 18/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0768 - accuracy: 0.9590 - val_loss: 0.1923 - val_accuracy: 0.9181\n",
            "Epoch 19/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0712 - accuracy: 0.9894 - val_loss: 0.2347 - val_accuracy: 0.8947\n",
            "Epoch 20/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0634 - accuracy: 0.9814 - val_loss: 0.2997 - val_accuracy: 0.8713\n",
            "Epoch 21/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0774 - accuracy: 0.9811 - val_loss: 0.2445 - val_accuracy: 0.8889\n",
            "Epoch 22/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0664 - accuracy: 0.9905 - val_loss: 0.2312 - val_accuracy: 0.9006\n",
            "Epoch 23/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0511 - accuracy: 0.9842 - val_loss: 0.3168 - val_accuracy: 0.8830\n",
            "Epoch 24/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0831 - accuracy: 0.9784 - val_loss: 0.2775 - val_accuracy: 0.8772\n",
            "Epoch 25/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0466 - accuracy: 0.9817 - val_loss: 0.2143 - val_accuracy: 0.9064\n",
            "Epoch 26/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.0911 - accuracy: 0.9742 - val_loss: 0.2240 - val_accuracy: 0.9123\n",
            "Epoch 27/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0540 - accuracy: 0.9763 - val_loss: 0.6183 - val_accuracy: 0.8480\n",
            "Epoch 28/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0666 - accuracy: 0.9733 - val_loss: 0.3511 - val_accuracy: 0.8713\n",
            "Epoch 29/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0388 - accuracy: 0.9907 - val_loss: 0.2595 - val_accuracy: 0.8947\n",
            "Epoch 30/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0344 - accuracy: 0.9949 - val_loss: 0.2434 - val_accuracy: 0.8889\n",
            "Epoch 31/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0302 - accuracy: 0.9948 - val_loss: 0.2909 - val_accuracy: 0.8947\n",
            "Epoch 32/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0257 - accuracy: 0.9941 - val_loss: 0.3244 - val_accuracy: 0.8830\n",
            "Epoch 33/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0887 - accuracy: 0.9622 - val_loss: 0.2540 - val_accuracy: 0.9123\n",
            "Epoch 34/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0430 - accuracy: 0.9858 - val_loss: 0.1607 - val_accuracy: 0.9298\n",
            "Epoch 35/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0512 - accuracy: 0.9849 - val_loss: 0.3342 - val_accuracy: 0.8889\n",
            "Epoch 36/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9991 - val_loss: 0.2033 - val_accuracy: 0.9181\n",
            "Epoch 37/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0439 - accuracy: 0.9701 - val_loss: 0.2252 - val_accuracy: 0.9240\n",
            "Epoch 38/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0362 - accuracy: 0.9929 - val_loss: 0.2021 - val_accuracy: 0.9240\n",
            "Epoch 39/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.9982 - val_loss: 0.2346 - val_accuracy: 0.9181\n",
            "Epoch 40/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0358 - accuracy: 0.9884 - val_loss: 0.2087 - val_accuracy: 0.9415\n",
            "Epoch 41/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0317 - accuracy: 0.9890 - val_loss: 0.5301 - val_accuracy: 0.8830\n",
            "Epoch 42/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0925 - accuracy: 0.9600 - val_loss: 0.2494 - val_accuracy: 0.9181\n",
            "Epoch 43/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.9930 - val_loss: 0.1461 - val_accuracy: 0.9532\n",
            "Epoch 44/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0461 - accuracy: 0.9705 - val_loss: 0.2178 - val_accuracy: 0.9298\n",
            "Epoch 45/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 0.9923 - val_loss: 0.2646 - val_accuracy: 0.9240\n",
            "Epoch 46/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0791 - accuracy: 0.9727 - val_loss: 0.3073 - val_accuracy: 0.9181\n",
            "Epoch 47/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0285 - accuracy: 0.9827 - val_loss: 0.1898 - val_accuracy: 0.9240\n",
            "Epoch 48/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0297 - accuracy: 0.9938 - val_loss: 0.1973 - val_accuracy: 0.9181\n",
            "Epoch 49/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0207 - accuracy: 0.9916 - val_loss: 0.1922 - val_accuracy: 0.9181\n",
            "Epoch 50/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0710 - accuracy: 0.9721 - val_loss: 0.2635 - val_accuracy: 0.9240\n",
            "Epoch 51/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0180 - accuracy: 0.9980 - val_loss: 0.2890 - val_accuracy: 0.9181\n",
            "Epoch 52/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0341 - accuracy: 0.9940 - val_loss: 0.1637 - val_accuracy: 0.9415\n",
            "Epoch 53/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0360 - accuracy: 0.9876 - val_loss: 0.1992 - val_accuracy: 0.9298\n",
            "Epoch 54/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0309 - accuracy: 0.9932 - val_loss: 0.2001 - val_accuracy: 0.9298\n",
            "Epoch 55/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9879 - val_loss: 0.2071 - val_accuracy: 0.9298\n",
            "Epoch 56/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 0.9984 - val_loss: 0.3513 - val_accuracy: 0.8947\n",
            "Epoch 57/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0459 - accuracy: 0.9782 - val_loss: 0.2210 - val_accuracy: 0.9415\n",
            "Epoch 58/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0137 - accuracy: 0.9940 - val_loss: 0.1674 - val_accuracy: 0.9357\n",
            "Epoch 59/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0245 - accuracy: 0.9886 - val_loss: 0.1978 - val_accuracy: 0.9415\n",
            "Epoch 60/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.9921 - val_loss: 0.2202 - val_accuracy: 0.9415\n",
            "Epoch 61/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.9884 - val_loss: 0.2033 - val_accuracy: 0.9415\n",
            "Epoch 62/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0173 - accuracy: 0.9899 - val_loss: 0.1670 - val_accuracy: 0.9357\n",
            "Epoch 63/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0341 - accuracy: 0.9877 - val_loss: 0.5780 - val_accuracy: 0.8655\n",
            "Epoch 64/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1116 - accuracy: 0.9614 - val_loss: 0.2524 - val_accuracy: 0.9240\n",
            "Epoch 65/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0361 - accuracy: 0.9741 - val_loss: 0.4212 - val_accuracy: 0.9064\n",
            "Epoch 66/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0197 - accuracy: 0.9987 - val_loss: 0.2457 - val_accuracy: 0.9181\n",
            "Epoch 67/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.9940 - val_loss: 0.1625 - val_accuracy: 0.9474\n",
            "Epoch 68/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0346 - accuracy: 0.9813 - val_loss: 0.2427 - val_accuracy: 0.9357\n",
            "Epoch 69/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0400 - accuracy: 0.9849 - val_loss: 0.6471 - val_accuracy: 0.8713\n",
            "Epoch 70/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1545 - accuracy: 0.9661 - val_loss: 0.2905 - val_accuracy: 0.9181\n",
            "Epoch 71/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9889 - val_loss: 0.1720 - val_accuracy: 0.9415\n",
            "Epoch 72/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 0.9914 - val_loss: 0.2866 - val_accuracy: 0.9123\n",
            "Epoch 73/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0183 - accuracy: 0.9982 - val_loss: 0.1616 - val_accuracy: 0.9415\n",
            "Epoch 74/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.9892 - val_loss: 0.1895 - val_accuracy: 0.9474\n",
            "Epoch 75/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9979 - val_loss: 0.2084 - val_accuracy: 0.9415\n",
            "Epoch 76/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 0.9946 - val_loss: 0.1920 - val_accuracy: 0.9415\n",
            "Epoch 77/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.9964 - val_loss: 0.3400 - val_accuracy: 0.9181\n",
            "Epoch 78/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 0.9897 - val_loss: 0.4460 - val_accuracy: 0.8889\n",
            "Epoch 79/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0611 - accuracy: 0.9811 - val_loss: 0.6586 - val_accuracy: 0.8830\n",
            "Epoch 80/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1621 - accuracy: 0.9377 - val_loss: 0.2931 - val_accuracy: 0.9240\n",
            "Epoch 81/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.9972 - val_loss: 0.3113 - val_accuracy: 0.9064\n",
            "Epoch 82/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0133 - accuracy: 0.9925 - val_loss: 0.1539 - val_accuracy: 0.9415\n",
            "Epoch 83/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0397 - accuracy: 0.9909 - val_loss: 0.2055 - val_accuracy: 0.9415\n",
            "Epoch 84/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0211 - accuracy: 0.9932 - val_loss: 0.2605 - val_accuracy: 0.9357\n",
            "Epoch 85/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 0.9964 - val_loss: 0.1686 - val_accuracy: 0.9474\n",
            "Epoch 86/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0349 - accuracy: 0.9866 - val_loss: 0.4039 - val_accuracy: 0.9064\n",
            "Epoch 87/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0995 - accuracy: 0.9617 - val_loss: 0.2112 - val_accuracy: 0.9415\n",
            "Epoch 88/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0168 - accuracy: 0.9924 - val_loss: 0.1731 - val_accuracy: 0.9415\n",
            "Epoch 89/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0176 - accuracy: 0.9903 - val_loss: 0.2956 - val_accuracy: 0.9298\n",
            "Epoch 90/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0255 - accuracy: 0.9874 - val_loss: 0.2011 - val_accuracy: 0.9415\n",
            "Epoch 91/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.9870 - val_loss: 0.2751 - val_accuracy: 0.9298\n",
            "Epoch 92/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.9958 - val_loss: 0.3655 - val_accuracy: 0.9064\n",
            "Epoch 93/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9973 - val_loss: 0.1962 - val_accuracy: 0.9474\n",
            "Epoch 94/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.9834 - val_loss: 0.1678 - val_accuracy: 0.9415\n",
            "Epoch 95/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.9939 - val_loss: 0.2658 - val_accuracy: 0.9415\n",
            "Epoch 96/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0259 - accuracy: 0.9896 - val_loss: 0.7090 - val_accuracy: 0.8713\n",
            "Epoch 97/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0754 - accuracy: 0.9710 - val_loss: 0.2644 - val_accuracy: 0.9357\n",
            "Epoch 98/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0923 - accuracy: 0.9571 - val_loss: 0.2427 - val_accuracy: 0.9357\n",
            "Epoch 99/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0147 - accuracy: 0.9940 - val_loss: 0.4183 - val_accuracy: 0.9181\n",
            "Epoch 100/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0201 - accuracy: 0.9949 - val_loss: 0.5031 - val_accuracy: 0.9181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f447e236d90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obIrKhbzEqgG",
        "outputId": "308a7f11-3fb5-4204-fb34-7d15e484fd6e"
      },
      "source": [
        "scores=model.evaluate(x,y)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1897 - accuracy: 0.9632\n",
            "accuracy: 96.32%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJV0IRO3EqfV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR7V66vmFL19"
      },
      "source": [
        "# **Gas Turbine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1D_cYc4EqaW"
      },
      "source": [
        "df=pd.read_csv(\"gas_turbines.csv\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "u5LzyTWkEqZd",
        "outputId": "436a9f6f-c766-4021-d984-62e9fac09494"
      },
      "source": [
        "df"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AT</th>\n",
              "      <th>AP</th>\n",
              "      <th>AH</th>\n",
              "      <th>AFDP</th>\n",
              "      <th>GTEP</th>\n",
              "      <th>TIT</th>\n",
              "      <th>TAT</th>\n",
              "      <th>TEY</th>\n",
              "      <th>CDP</th>\n",
              "      <th>CO</th>\n",
              "      <th>NOX</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.8594</td>\n",
              "      <td>1007.9</td>\n",
              "      <td>96.799</td>\n",
              "      <td>3.5000</td>\n",
              "      <td>19.663</td>\n",
              "      <td>1059.2</td>\n",
              "      <td>550.00</td>\n",
              "      <td>114.70</td>\n",
              "      <td>10.605</td>\n",
              "      <td>3.1547</td>\n",
              "      <td>82.722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.7850</td>\n",
              "      <td>1008.4</td>\n",
              "      <td>97.118</td>\n",
              "      <td>3.4998</td>\n",
              "      <td>19.728</td>\n",
              "      <td>1059.3</td>\n",
              "      <td>550.00</td>\n",
              "      <td>114.72</td>\n",
              "      <td>10.598</td>\n",
              "      <td>3.2363</td>\n",
              "      <td>82.776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.8977</td>\n",
              "      <td>1008.8</td>\n",
              "      <td>95.939</td>\n",
              "      <td>3.4824</td>\n",
              "      <td>19.779</td>\n",
              "      <td>1059.4</td>\n",
              "      <td>549.87</td>\n",
              "      <td>114.71</td>\n",
              "      <td>10.601</td>\n",
              "      <td>3.2012</td>\n",
              "      <td>82.468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.0569</td>\n",
              "      <td>1009.2</td>\n",
              "      <td>95.249</td>\n",
              "      <td>3.4805</td>\n",
              "      <td>19.792</td>\n",
              "      <td>1059.6</td>\n",
              "      <td>549.99</td>\n",
              "      <td>114.72</td>\n",
              "      <td>10.606</td>\n",
              "      <td>3.1923</td>\n",
              "      <td>82.670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.3978</td>\n",
              "      <td>1009.7</td>\n",
              "      <td>95.150</td>\n",
              "      <td>3.4976</td>\n",
              "      <td>19.765</td>\n",
              "      <td>1059.7</td>\n",
              "      <td>549.98</td>\n",
              "      <td>114.72</td>\n",
              "      <td>10.612</td>\n",
              "      <td>3.2484</td>\n",
              "      <td>82.311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15034</th>\n",
              "      <td>9.0301</td>\n",
              "      <td>1005.6</td>\n",
              "      <td>98.460</td>\n",
              "      <td>3.5421</td>\n",
              "      <td>19.164</td>\n",
              "      <td>1049.7</td>\n",
              "      <td>546.21</td>\n",
              "      <td>111.61</td>\n",
              "      <td>10.400</td>\n",
              "      <td>4.5186</td>\n",
              "      <td>79.559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15035</th>\n",
              "      <td>7.8879</td>\n",
              "      <td>1005.9</td>\n",
              "      <td>99.093</td>\n",
              "      <td>3.5059</td>\n",
              "      <td>19.414</td>\n",
              "      <td>1046.3</td>\n",
              "      <td>543.22</td>\n",
              "      <td>111.78</td>\n",
              "      <td>10.433</td>\n",
              "      <td>4.8470</td>\n",
              "      <td>79.917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15036</th>\n",
              "      <td>7.2647</td>\n",
              "      <td>1006.3</td>\n",
              "      <td>99.496</td>\n",
              "      <td>3.4770</td>\n",
              "      <td>19.530</td>\n",
              "      <td>1037.7</td>\n",
              "      <td>537.32</td>\n",
              "      <td>110.19</td>\n",
              "      <td>10.483</td>\n",
              "      <td>7.9632</td>\n",
              "      <td>90.912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15037</th>\n",
              "      <td>7.0060</td>\n",
              "      <td>1006.8</td>\n",
              "      <td>99.008</td>\n",
              "      <td>3.4486</td>\n",
              "      <td>19.377</td>\n",
              "      <td>1043.2</td>\n",
              "      <td>541.24</td>\n",
              "      <td>110.74</td>\n",
              "      <td>10.533</td>\n",
              "      <td>6.2494</td>\n",
              "      <td>93.227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15038</th>\n",
              "      <td>6.9279</td>\n",
              "      <td>1007.2</td>\n",
              "      <td>97.533</td>\n",
              "      <td>3.4275</td>\n",
              "      <td>19.306</td>\n",
              "      <td>1049.9</td>\n",
              "      <td>545.85</td>\n",
              "      <td>111.58</td>\n",
              "      <td>10.583</td>\n",
              "      <td>4.9816</td>\n",
              "      <td>92.498</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15039 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           AT      AP      AH    AFDP  ...     TEY     CDP      CO     NOX\n",
              "0      6.8594  1007.9  96.799  3.5000  ...  114.70  10.605  3.1547  82.722\n",
              "1      6.7850  1008.4  97.118  3.4998  ...  114.72  10.598  3.2363  82.776\n",
              "2      6.8977  1008.8  95.939  3.4824  ...  114.71  10.601  3.2012  82.468\n",
              "3      7.0569  1009.2  95.249  3.4805  ...  114.72  10.606  3.1923  82.670\n",
              "4      7.3978  1009.7  95.150  3.4976  ...  114.72  10.612  3.2484  82.311\n",
              "...       ...     ...     ...     ...  ...     ...     ...     ...     ...\n",
              "15034  9.0301  1005.6  98.460  3.5421  ...  111.61  10.400  4.5186  79.559\n",
              "15035  7.8879  1005.9  99.093  3.5059  ...  111.78  10.433  4.8470  79.917\n",
              "15036  7.2647  1006.3  99.496  3.4770  ...  110.19  10.483  7.9632  90.912\n",
              "15037  7.0060  1006.8  99.008  3.4486  ...  110.74  10.533  6.2494  93.227\n",
              "15038  6.9279  1007.2  97.533  3.4275  ...  111.58  10.583  4.9816  92.498\n",
              "\n",
              "[15039 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEzjhAMQFkLL",
        "outputId": "c4e22f3f-f3ec-4ed6-b55d-e93a86ccae3a"
      },
      "source": [
        "df1=df.values\n",
        "df1"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   6.8594, 1007.9   ,   96.799 , ...,   10.605 ,    3.1547,\n",
              "          82.722 ],\n",
              "       [   6.785 , 1008.4   ,   97.118 , ...,   10.598 ,    3.2363,\n",
              "          82.776 ],\n",
              "       [   6.8977, 1008.8   ,   95.939 , ...,   10.601 ,    3.2012,\n",
              "          82.468 ],\n",
              "       ...,\n",
              "       [   7.2647, 1006.3   ,   99.496 , ...,   10.483 ,    7.9632,\n",
              "          90.912 ],\n",
              "       [   7.006 , 1006.8   ,   99.008 , ...,   10.533 ,    6.2494,\n",
              "          93.227 ],\n",
              "       [   6.9279, 1007.2   ,   97.533 , ...,   10.583 ,    4.9816,\n",
              "          92.498 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkJGVgQhFkKe",
        "outputId": "c43361e0-7d78-4ac9-f537-729ccf16aa90"
      },
      "source": [
        "X=df1[:,[0,1,2,3,4,5,6,8,9,10]]\n",
        "Y=df1[:,-4]\n",
        "X"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   6.8594, 1007.9   ,   96.799 , ...,   10.605 ,    3.1547,\n",
              "          82.722 ],\n",
              "       [   6.785 , 1008.4   ,   97.118 , ...,   10.598 ,    3.2363,\n",
              "          82.776 ],\n",
              "       [   6.8977, 1008.8   ,   95.939 , ...,   10.601 ,    3.2012,\n",
              "          82.468 ],\n",
              "       ...,\n",
              "       [   7.2647, 1006.3   ,   99.496 , ...,   10.483 ,    7.9632,\n",
              "          90.912 ],\n",
              "       [   7.006 , 1006.8   ,   99.008 , ...,   10.533 ,    6.2494,\n",
              "          93.227 ],\n",
              "       [   6.9279, 1007.2   ,   97.533 , ...,   10.583 ,    4.9816,\n",
              "          92.498 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqqcddmwFkHF"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g1qhUIfFkCY"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.25,random_state=101)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GDBJng8Fj9-"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkWZpFkDFj8O",
        "outputId": "80b5e86f-4d16-4014-921f-08cd1dd9dccf"
      },
      "source": [
        "scaler=MinMaxScaler()\n",
        "scaler.fit(x_train)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MinMaxScaler(copy=True, feature_range=(0, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LdpMVNkGC7H",
        "outputId": "5b73dbc0-cc11-449e-bd40-f4078efac530"
      },
      "source": [
        "x_train=scaler.transform(x_train)\n",
        "x_test=scaler.transform(x_test)\n",
        "x_test"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.35890393, 0.40602285, 0.91801706, ..., 0.34107329, 0.03084967,\n",
              "        0.48475958],\n",
              "       [0.55162803, 0.59086189, 0.72785444, ..., 0.42819611, 0.02833486,\n",
              "        0.43366477],\n",
              "       [0.69430373, 0.53478712, 0.55215014, ..., 0.14847583, 0.15186537,\n",
              "        0.33822331],\n",
              "       ...,\n",
              "       [0.29923532, 0.48494289, 0.94876603, ..., 0.77514199, 0.00101504,\n",
              "        0.41400706],\n",
              "       [0.64399376, 0.35825545, 0.50904718, ..., 0.04705791, 0.10100297,\n",
              "        0.36756316],\n",
              "       [0.3486443 , 0.24340602, 0.81637941, ..., 0.34416412, 0.00787964,\n",
              "        0.54170062]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhekVuokGC5l"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "# add nodes for prediction\n",
        "model.add(Dense(1))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2bEt6ZGGC2M"
      },
      "source": [
        "model.compile(optimizer='rmsprop',loss='mse')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d6DG8b5GCxl",
        "outputId": "22b6d158-abd3-4719-e516-a1d452e781be"
      },
      "source": [
        "# Fit the model\n",
        "model.fit(x_train, y_train, epochs=250)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "353/353 [==============================] - 9s 1ms/step - loss: 17044.9640\n",
            "Epoch 2/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1994.4178\n",
            "Epoch 3/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 31.4477\n",
            "Epoch 4/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 22.5500\n",
            "Epoch 5/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 18.5496\n",
            "Epoch 6/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 15.9628\n",
            "Epoch 7/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 13.6972\n",
            "Epoch 8/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 11.6350\n",
            "Epoch 9/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 10.2446\n",
            "Epoch 10/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 8.5106\n",
            "Epoch 11/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 7.5022\n",
            "Epoch 12/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 6.2617\n",
            "Epoch 13/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 5.2183\n",
            "Epoch 14/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 4.4700\n",
            "Epoch 15/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 3.5531\n",
            "Epoch 16/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 2.9011\n",
            "Epoch 17/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 2.3443\n",
            "Epoch 18/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.9567\n",
            "Epoch 19/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.6530\n",
            "Epoch 20/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.3332\n",
            "Epoch 21/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.1415\n",
            "Epoch 22/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.0760\n",
            "Epoch 23/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.0056\n",
            "Epoch 24/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9649\n",
            "Epoch 25/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9366\n",
            "Epoch 26/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9028\n",
            "Epoch 27/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8997\n",
            "Epoch 28/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8910\n",
            "Epoch 29/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8798\n",
            "Epoch 30/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9284\n",
            "Epoch 31/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8667\n",
            "Epoch 32/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9077\n",
            "Epoch 33/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8995\n",
            "Epoch 34/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8823\n",
            "Epoch 35/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8866\n",
            "Epoch 36/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8759\n",
            "Epoch 37/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9041\n",
            "Epoch 38/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9070\n",
            "Epoch 39/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8962\n",
            "Epoch 40/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9077\n",
            "Epoch 41/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8901\n",
            "Epoch 42/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9010\n",
            "Epoch 43/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8636\n",
            "Epoch 44/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8681\n",
            "Epoch 45/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8673\n",
            "Epoch 46/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8891\n",
            "Epoch 47/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8625\n",
            "Epoch 48/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8664\n",
            "Epoch 49/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8767\n",
            "Epoch 50/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8770\n",
            "Epoch 51/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8917\n",
            "Epoch 52/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9159\n",
            "Epoch 53/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8815\n",
            "Epoch 54/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8927\n",
            "Epoch 55/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9114\n",
            "Epoch 56/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8445\n",
            "Epoch 57/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8747\n",
            "Epoch 58/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8659\n",
            "Epoch 59/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8815\n",
            "Epoch 60/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8774\n",
            "Epoch 61/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8320\n",
            "Epoch 62/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8404\n",
            "Epoch 63/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8746\n",
            "Epoch 64/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8658\n",
            "Epoch 65/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8472\n",
            "Epoch 66/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8771\n",
            "Epoch 67/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8630\n",
            "Epoch 68/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8989\n",
            "Epoch 69/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8601\n",
            "Epoch 70/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8909\n",
            "Epoch 71/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8517\n",
            "Epoch 72/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8495\n",
            "Epoch 73/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8507\n",
            "Epoch 74/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8912\n",
            "Epoch 75/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8621\n",
            "Epoch 76/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8558\n",
            "Epoch 77/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8794\n",
            "Epoch 78/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8815\n",
            "Epoch 79/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8388\n",
            "Epoch 80/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8069\n",
            "Epoch 81/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8406\n",
            "Epoch 82/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8899\n",
            "Epoch 83/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8569\n",
            "Epoch 84/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8169\n",
            "Epoch 85/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8406\n",
            "Epoch 86/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8723\n",
            "Epoch 87/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8821\n",
            "Epoch 88/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8609\n",
            "Epoch 89/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8578\n",
            "Epoch 90/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8312\n",
            "Epoch 91/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8461\n",
            "Epoch 92/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8464\n",
            "Epoch 93/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8420\n",
            "Epoch 94/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8614\n",
            "Epoch 95/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8261\n",
            "Epoch 96/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8434\n",
            "Epoch 97/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8053\n",
            "Epoch 98/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8635\n",
            "Epoch 99/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8002\n",
            "Epoch 100/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8233\n",
            "Epoch 101/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8477\n",
            "Epoch 102/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8430\n",
            "Epoch 103/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8609\n",
            "Epoch 104/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8450\n",
            "Epoch 105/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8494\n",
            "Epoch 106/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8321\n",
            "Epoch 107/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8554\n",
            "Epoch 108/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8361\n",
            "Epoch 109/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8438\n",
            "Epoch 110/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8738\n",
            "Epoch 111/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8659\n",
            "Epoch 112/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8751\n",
            "Epoch 113/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8413\n",
            "Epoch 114/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8654\n",
            "Epoch 115/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8662\n",
            "Epoch 116/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8889\n",
            "Epoch 117/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8477\n",
            "Epoch 118/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8346\n",
            "Epoch 119/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8604\n",
            "Epoch 120/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8553\n",
            "Epoch 121/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7873\n",
            "Epoch 122/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8636\n",
            "Epoch 123/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8257\n",
            "Epoch 124/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8538\n",
            "Epoch 125/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8313\n",
            "Epoch 126/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8342\n",
            "Epoch 127/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8187\n",
            "Epoch 128/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8418\n",
            "Epoch 129/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8295\n",
            "Epoch 130/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8047\n",
            "Epoch 131/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8340\n",
            "Epoch 132/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8280\n",
            "Epoch 133/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8860\n",
            "Epoch 134/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8131\n",
            "Epoch 135/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8536\n",
            "Epoch 136/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8403\n",
            "Epoch 137/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8224\n",
            "Epoch 138/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8320\n",
            "Epoch 139/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8167\n",
            "Epoch 140/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8179\n",
            "Epoch 141/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8177\n",
            "Epoch 142/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8562\n",
            "Epoch 143/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8255\n",
            "Epoch 144/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8375\n",
            "Epoch 145/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7747\n",
            "Epoch 146/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8096\n",
            "Epoch 147/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8075\n",
            "Epoch 148/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8338\n",
            "Epoch 149/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8048\n",
            "Epoch 150/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8296\n",
            "Epoch 151/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8210\n",
            "Epoch 152/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8363\n",
            "Epoch 153/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8240\n",
            "Epoch 154/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8090\n",
            "Epoch 155/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7751\n",
            "Epoch 156/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8235\n",
            "Epoch 157/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8338\n",
            "Epoch 158/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8236\n",
            "Epoch 159/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8256\n",
            "Epoch 160/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7610\n",
            "Epoch 161/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7843\n",
            "Epoch 162/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8655\n",
            "Epoch 163/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8160\n",
            "Epoch 164/250\n",
            "353/353 [==============================] - 1s 1ms/step - loss: 0.8071\n",
            "Epoch 165/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7989\n",
            "Epoch 166/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7819\n",
            "Epoch 167/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7917\n",
            "Epoch 168/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8429\n",
            "Epoch 169/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7827\n",
            "Epoch 170/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8269\n",
            "Epoch 171/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7917\n",
            "Epoch 172/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8297\n",
            "Epoch 173/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7818\n",
            "Epoch 174/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8375\n",
            "Epoch 175/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8150\n",
            "Epoch 176/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7962\n",
            "Epoch 177/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8072\n",
            "Epoch 178/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8031\n",
            "Epoch 179/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8028\n",
            "Epoch 180/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7938\n",
            "Epoch 181/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8030\n",
            "Epoch 182/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8609\n",
            "Epoch 183/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8022\n",
            "Epoch 184/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8601\n",
            "Epoch 185/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8164\n",
            "Epoch 186/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7937\n",
            "Epoch 187/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8130\n",
            "Epoch 188/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8283\n",
            "Epoch 189/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8086\n",
            "Epoch 190/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7975\n",
            "Epoch 191/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7874\n",
            "Epoch 192/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7945\n",
            "Epoch 193/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8015\n",
            "Epoch 194/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7917\n",
            "Epoch 195/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7753\n",
            "Epoch 196/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8018\n",
            "Epoch 197/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7989\n",
            "Epoch 198/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8281\n",
            "Epoch 199/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7925\n",
            "Epoch 200/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8066\n",
            "Epoch 201/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8455\n",
            "Epoch 202/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7866\n",
            "Epoch 203/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8170\n",
            "Epoch 204/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7863\n",
            "Epoch 205/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8093\n",
            "Epoch 206/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7887\n",
            "Epoch 207/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7863\n",
            "Epoch 208/250\n",
            "353/353 [==============================] - 1s 2ms/step - loss: 0.7821\n",
            "Epoch 209/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8179\n",
            "Epoch 210/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7812\n",
            "Epoch 211/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7903\n",
            "Epoch 212/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8065\n",
            "Epoch 213/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7851\n",
            "Epoch 214/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7580\n",
            "Epoch 215/250\n",
            "353/353 [==============================] - 1s 1ms/step - loss: 0.7913\n",
            "Epoch 216/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7844\n",
            "Epoch 217/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7789\n",
            "Epoch 218/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8185\n",
            "Epoch 219/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8087\n",
            "Epoch 220/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7649\n",
            "Epoch 221/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7929\n",
            "Epoch 222/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7691\n",
            "Epoch 223/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8192\n",
            "Epoch 224/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8034\n",
            "Epoch 225/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7824\n",
            "Epoch 226/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7804\n",
            "Epoch 227/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8041\n",
            "Epoch 228/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7678\n",
            "Epoch 229/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7976\n",
            "Epoch 230/250\n",
            "353/353 [==============================] - 1s 1ms/step - loss: 0.7978\n",
            "Epoch 231/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8067\n",
            "Epoch 232/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7605\n",
            "Epoch 233/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8124\n",
            "Epoch 234/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7954\n",
            "Epoch 235/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7965\n",
            "Epoch 236/250\n",
            "353/353 [==============================] - 1s 1ms/step - loss: 0.7888\n",
            "Epoch 237/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7769\n",
            "Epoch 238/250\n",
            "353/353 [==============================] - 1s 1ms/step - loss: 0.7620\n",
            "Epoch 239/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7786\n",
            "Epoch 240/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7817\n",
            "Epoch 241/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7900\n",
            "Epoch 242/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7755\n",
            "Epoch 243/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7846\n",
            "Epoch 244/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7983\n",
            "Epoch 245/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7917\n",
            "Epoch 246/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7882\n",
            "Epoch 247/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8034\n",
            "Epoch 248/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8073\n",
            "Epoch 249/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8055\n",
            "Epoch 250/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7874\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f447d9c14d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "tMEPdFMCGgIU",
        "outputId": "012b3cb4-984a-47a3-8fe6-63715fdfb8e5"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib as plot\n",
        "model_loss = pd.DataFrame(model.history.history)\n",
        "model_loss.plot()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f447d2b2f50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZiW-AxXGgE9"
      },
      "source": [
        "pred = model.predict(x_test)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXMXNQmkGgD3",
        "outputId": "21c9c272-6a51-4295-c3f0-7c22b69525e3"
      },
      "source": [
        "pred"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[132.49333 ],\n",
              "       [133.81648 ],\n",
              "       [111.055305],\n",
              "       ...,\n",
              "       [160.37393 ],\n",
              "       [104.51492 ],\n",
              "       [132.82033 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXtkP1SBGgCc"
      },
      "source": [
        "pred = pred.ravel()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUJFrRhBGf-8",
        "outputId": "b8ca1b48-5727-4e0a-bfe6-e2e80f30ce92"
      },
      "source": [
        "test_score = model.evaluate(x_test,y_test,verbose=0)\n",
        "test_score"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5242202877998352"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb1Sx1YFGf7u"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error,mean_squared_error"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5npqyS6cGzlk",
        "outputId": "546431af-ff44-4efc-e4b3-a6e31b820caa"
      },
      "source": [
        "mean_absolute_error(pred,y_test)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5313330875964872"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5-zyZIDGzhG",
        "outputId": "d7c8914a-9d64-4cf1-acd4-03df0d826748"
      },
      "source": [
        "mean_squared_error(pred,y_test)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5242203054174909"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aZ86y6YGzcy"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Nxgoozs8GzXc",
        "outputId": "63d38a81-a489-46f4-cb0a-32626cc497b4"
      },
      "source": [
        "plt.scatter(y_test,pred)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f447d85d4d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}